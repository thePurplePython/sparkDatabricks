{"cells":[{"cell_type":"markdown","source":["# Streaming ETL on CloudTrail Logs using Structured Streaming\nIn this Python notebook, we are going to explore how we can use Structured Streaming to perform streaming ETL on CloudTrail logs. For more context, read the Databricks blog. \n\n[AWS CloudTrail](https://aws.amazon.com/cloudtrail/) is a web service that records AWS API calls for your account and delivers audit logs to you as JSON files in a S3 bucket. If you do not have it configured, see their documentations on how to do so."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Where is your input data and where do you want your final Parquet table?\nTo run this notebook, first of all, you need to specify the location of the CloudTrail logs files. You can open your CloudTrail configuration, find the bucket and set the value below."],"metadata":{}},{"cell_type":"code","source":["cloudTrailLogsPath = \"s3n://MY_CLOUDTRAIL_BUCKET/AWSLogs/*/CloudTrail/*/2017/01/03/\"\nparquetOutputPath = \"/MY_OUTPUT_PATH\"  # DBFS or S3 path "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Note that this uses globs to read logs across all the accounts and all the AWS regions that are being reported to the bucket. If you want to limit your processing to a smaller subset of the logs, change the above path accordingly."],"metadata":{}},{"cell_type":"markdown","source":["### Step 2: What is the schema of your data?\nTo parse the JSON files, we need to know schema of the JSON data in the log files. Below is the schema defined based on the format defined in [CloudTrail documentation](http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference.html). It is essentially an array (named Records) of fields related to events, some of which are nested structures."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.streaming import ProcessingTime\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\ncloudTrailSchema = StructType() \\\n  .add(\"Records\", ArrayType(StructType() \\\n    .add(\"additionalEventData\", StringType()) \\\n    .add(\"apiVersion\", StringType()) \\\n    .add(\"awsRegion\", StringType()) \\\n    .add(\"errorCode\", StringType()) \\\n    .add(\"errorMessage\", StringType()) \\\n    .add(\"eventID\", StringType()) \\\n    .add(\"eventName\", StringType()) \\\n    .add(\"eventSource\", StringType()) \\\n    .add(\"eventTime\", StringType()) \\\n    .add(\"eventType\", StringType()) \\\n    .add(\"eventVersion\", StringType()) \\\n    .add(\"readOnly\", BooleanType()) \\\n    .add(\"recipientAccountId\", StringType()) \\\n    .add(\"requestID\", StringType()) \\\n    .add(\"requestParameters\", MapType(StringType(), StringType())) \\\n    .add(\"resources\", ArrayType(StructType() \\\n      .add(\"ARN\", StringType()) \\\n      .add(\"accountId\", StringType()) \\\n      .add(\"type\", StringType()) \\\n    )) \\\n    .add(\"responseElements\", MapType(StringType(), StringType())) \\\n    .add(\"sharedEventID\", StringType()) \\\n    .add(\"sourceIPAddress\", StringType()) \\\n    .add(\"serviceEventDetails\", MapType(StringType(), StringType())) \\\n    .add(\"userAgent\", StringType()) \\\n    .add(\"userIdentity\", StructType() \\\n      .add(\"accessKeyId\", StringType()) \\\n      .add(\"accountId\", StringType()) \\\n      .add(\"arn\", StringType()) \\\n      .add(\"invokedBy\", StringType()) \\\n      .add(\"principalId\", StringType()) \\\n      .add(\"sessionContext\", StructType() \\\n        .add(\"attributes\", StructType() \\\n          .add(\"creationDate\", StringType()) \\\n          .add(\"mfaAuthenticated\", StringType()) \\\n        ) \\\n        .add(\"sessionIssuer\", StructType() \\\n          .add(\"accountId\", StringType()) \\\n          .add(\"arn\", StringType()) \\\n          .add(\"principalId\", StringType()) \\\n          .add(\"type\", StringType()) \\\n          .add(\"userName\", StringType()) \\\n        )\n      ) \\\n      .add(\"type\", StringType()) \\\n      .add(\"userName\", StringType()) \\\n      .add(\"webIdFederationData\", StructType() \\\n        .add(\"federatedProvider\", StringType()) \\\n        .add(\"attributes\", MapType(StringType(), StringType())) \\\n      )\n    ) \\\n    .add(\"vpcEndpointId\", StringType())))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Step 3: Let's do streaming ETL on it!\nNow, we can start reading the data and writing to Parquet table. First, we are going to create the streaming DataFrame that represents the raw records in the files, using the schema we have defined.\nWe are also option `maxFilesPerTrigger` to get earlier access the final Parquet data, as this limit the number of log files processed and written out every trigger."],"metadata":{}},{"cell_type":"code","source":["rawRecords = spark.readStream \\\n  .option(\"maxFilesPerTrigger\", \"100\") \\\n  .schema(cloudTrailSchema) \\\n  .json(cloudTrailLogsPath)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Then, we are going to transform the data in the following way.\n\n1. `Explode` (split) the array of records loaded from each file into separate records.\n2. Parse the string event time string in each record to Sparkâ€™s timestamp type.\n3. Flatten out the nested columns for easier querying."],"metadata":{}},{"cell_type":"code","source":["cloudTrailEvents = rawRecords \\\n  .select(explode(\"Records\").alias(\"record\")) \\\n  .select(\n    unix_timestamp(\"record.eventTime\", \"yyyy-MM-dd'T'hh:mm:ss\").cast(\"timestamp\").alias(\"timestamp\"),\n    \"record.*\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Finally, we can define how to write out the transformed data and start the `StreamingQuery`. We are going to do the following\n\n- Write the data out in the Parquet format, \n- Define the `date` column from that `timestamp` and partition the Parquet data by date for efficient time-slice queries.\n- Define the trigger to be every 10 seconds.\n- Define the checkpoint location\n- Finally, start the query"],"metadata":{}},{"cell_type":"code","source":["checkpointPath = \"/cloudtrail.checkpoint/\n\nstreamingETLQuery = cloudTrailEvents \\\n  .withColumn(\"date\", cloudTrailEvents.timestamp.cast(\"date\")) \\\n  .writeStream \\\n  .format(\"parquet\") \\\n  .option(\"path\", parquetOutputPath) \\\n  .partitionBy(\"date\") \\\n  .trigger(processingTime=\"10 seconds\") \\\n  .option(\"checkpointLocation\", checkpointPath) \\\n  .start()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The `streamingETLQuery` should be running in the background. See the `Details` to understand the progress."],"metadata":{}},{"cell_type":"markdown","source":["### Step 4: Query up-to-the-minute data from Parquet Table\n\nWhile the `streamingETLQuery` is continuously converting the data to Parquet, you can already start running ad-hoc queries on the Parquet table. Your queries will always pick up the latest written files while ensuring data consistency."],"metadata":{}},{"cell_type":"code","source":["parquetData = sql(\"select * from parquet.`{}`\".format(parquetOutputPath))\ndisplay(parquetData)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["If you count the number of rows in the table, you should find the value increasing over time. Run the following every few minutes."],"metadata":{}},{"cell_type":"code","source":["sql(\"select * from parquet.`{}`\".format(parquetOutputPath)).count()"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"Cloudtrail ETL - Python","notebookId":2662280497220778},"nbformat":4,"nbformat_minor":0}
